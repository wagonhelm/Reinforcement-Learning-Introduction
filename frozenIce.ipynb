{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1><u>Solving Frozen Lake Environment</h1></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-16 17:01:01,076] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "print(n_states)\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>How good does performing randomly do?</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached goal after 68 episodes with a average return of 0.014705882352941176\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "reward = None\n",
    "done = None\n",
    "\n",
    "g = 0\n",
    "episodes = 0\n",
    "rewardTracker = []\n",
    "\n",
    "while reward != 1:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    g += reward\n",
    "    if done == True:\n",
    "        rewardTracker.append(g)\n",
    "        state = env.reset()\n",
    "        episodes += 1\n",
    "print(\"Reached goal after {} episodes with a average return of {}\".format(episodes, sum(rewardTracker)/len(rewardTracker)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>How good does previously used Q Learning work?</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return of 0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "rewardTracker = []\n",
    "\n",
    "Q = np.zeros([n_states, n_states])\n",
    "\n",
    "G = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "    state = env.reset()\n",
    "    while done != True:\n",
    "        action = np.argmax(Q[state]) \n",
    "        state2, reward, done, info = env.step(action) \n",
    "        Q[state,action] += alpha * ((reward + (np.max(Q[state2]))  - Q[state,action]))\n",
    "        G += reward\n",
    "        state = state2\n",
    "    rewardTracker.append(G)\n",
    "    \n",
    "print(\"Average return of {}\".format(sum(rewardTracker)/len(rewardTracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Since we do not receive a negative reward for each action, all our agent ever does is try action 0 and doesn't explore other actions</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>We can create a epsilon greedy policy that will choose a random action at a given epsilon percentage</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return of 0.0356\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.5 \n",
    "episodes = 5000\n",
    "rewardTracker = []\n",
    "\n",
    "Q = np.zeros([n_states, n_states])\n",
    "\n",
    "G = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "    state = env.reset()\n",
    "    while done != True:\n",
    "        \n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(Q[state])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        state2, reward, done, info = env.step(action) \n",
    "        Q[state,action] += alpha * ((reward + (np.max(Q[state2]))  - Q[state,action]))\n",
    "        G += reward\n",
    "        state = state2\n",
    "    rewardTracker.append(G)\n",
    "    \n",
    "print(\"Average return of {}\".format(sum(rewardTracker)/len(rewardTracker)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Still not very impressive</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Let's add reward discounting and a decaying epsilon</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return of 0.4158\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1\n",
    "gamma = 0.95\n",
    "\n",
    "rewardTracker = []\n",
    "Q = np.zeros([n_states, n_states])\n",
    "episodes = 5000\n",
    "G = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "    state = env.reset()\n",
    "    while done != True:\n",
    "        \n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(Q[state])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            epsilon -= 10**-3\n",
    "            \n",
    "        state2, reward, done, info = env.step(action) \n",
    "        Q[state,action] += alpha * ((reward + gamma * (np.max(Q[state2])) -  Q[state,action]))\n",
    "        G += reward\n",
    "        state = state2\n",
    "    rewardTracker.append(G)\n",
    "\n",
    "if (sum(rewardTracker[episode-100:episode])/100.0) > .78:\n",
    "            print('-------------------------------------------------------')\n",
    "            print('Solved after {} episodes with average return of {}'.format(episode-100, sum(rewardTracker[episode-100:episodeNum])/100.0))\n",
    "    \n",
    "print(\"Average return of {}\".format(sum(rewardTracker)/len(rewardTracker)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>We are getting closer but we need to define a more appropriate epsilon greedy policy</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_greedy(eps, Q, state, episode):\n",
    "    \n",
    "    if np.random.rand() > eps:\n",
    "        action = np.argmax(Q[state,:]+np.random.randn(1, n_actions)/(episode/4))\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        eps -= 10**-5\n",
    "\n",
    "    return action, eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Much like other machine learnings problems you need to define optimal hyper parameters</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_Q(alpha, gamma, eps, numTrainingEpisodes, numTrainingSteps):\n",
    "\n",
    "    global Q_star\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    rewardTracker = []\n",
    "    \n",
    "    for episode in range(1,numTrainingEpisodes+1):  \n",
    "        \n",
    "        G = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        for step in range(1,numTrainingSteps):\n",
    "            \n",
    "            action, eps = e_greedy(eps, Q, state, episode)\n",
    "            state2, reward, done, info = env.step(action)\n",
    "            Q[state,action] += alpha * (reward + gamma * np.max(Q[state2]) - Q[state,action])\n",
    "            state = state2\n",
    "            G += reward\n",
    "        \n",
    "        rewardTracker.append(G)\n",
    "        \n",
    "        if episode % (numTrainingEpisodes*.10) == 0 and episode != 0:\n",
    "            print('Alpha {}  Gamma {}  Epsilon {:04.3f}  Episode {} of {}'.format(alpha, gamma, eps, episode, numTrainingEpisodes))\n",
    "            print(\"Average Total Return: {}\".format(sum(rewardTracker)/episode))\n",
    "        \n",
    "        if (sum(rewardTracker[episode-100:episode])/100.0) > .78:\n",
    "            print('-------------------------------------------------------')\n",
    "            print('Solved after {} episodes with average return of {}'.format(episode-100, sum(rewardTracker[episode-100:episode])/100.0))\n",
    "            Q_star = Q\n",
    "            break\n",
    "    Q_star = Q\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha 0.8  Gamma 0.95  Epsilon 0.022  Episode 500 of 5000\n",
      "Average Total Return: 0.236\n",
      "Alpha 0.8  Gamma 0.95  Epsilon 0.005  Episode 1000 of 5000\n",
      "Average Total Return: 0.318\n",
      "Alpha 0.8  Gamma 0.95  Epsilon 0.001  Episode 1500 of 5000\n",
      "Average Total Return: 0.40066666666666667\n",
      "-------------------------------------------------------\n",
      "Solved after 1409 episodes with average return of 0.79\n"
     ]
    }
   ],
   "source": [
    "# Alpha, Gamma, Eps, Episodes, Steps per Episode\n",
    "learn_Q(0.8, 0.95, 0.1, 5000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(Q, numTrainingEpisodes, numTrainingSteps, render):\n",
    "\n",
    "    rewardTracker = []\n",
    "    \n",
    "    for episode in range(1,numTrainingEpisodes+1):  \n",
    "        \n",
    "        G = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        for step in range(1,numTrainingSteps):\n",
    "            \n",
    "            action = np.argmax(Q[state])\n",
    "        \n",
    "            state2, reward, done, info = env.step(action)\n",
    "            state = state2\n",
    "            G += reward\n",
    "            if render == True:\n",
    "                env.render()\n",
    "            \n",
    "            if done == True:\n",
    "                break\n",
    "                \n",
    "        rewardTracker.append(G)        \n",
    "    \n",
    "        if episode % (numTrainingEpisodes*.10) == 0 and episode != 0:\n",
    "    \n",
    "            print(\"Average Total Return After {} Episodes: {:04.3f}\".format(episode, sum(rewardTracker)/episode))\n",
    "            \n",
    "   \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Q-Table, Number of Episodes, Number of Steps per Episode, Render\n",
    "evaluate(Q_star, 1, 300, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
